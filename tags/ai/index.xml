<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on 局域自由博客</title>
        <link>https://localfreedom.github.io/tags/ai/</link>
        <description>Recent content in AI on 局域自由博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 27 Mar 2025 10:03:25 +0000</lastBuildDate><atom:link href="https://localfreedom.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>LM Studio</title>
        <link>https://localfreedom.github.io/p/lm-studio/</link>
        <pubDate>Thu, 27 Mar 2025 10:03:25 +0000</pubDate>
        
        <guid>https://localfreedom.github.io/p/lm-studio/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;LM Studio 是一个桌面应用程序，它允许你轻松下载、管理和运行各种GGUF格式的开源 LLM，无需连接互联网。
它提供了一个非常友好的配置界面，让你能够探索huggingface，快速体验不同模型的特性，并根据自己的需求进行调整。
在对各类新型模型的支持中，LM Studio 总是非常迅速完美的。
不过 LM Studio 是一个免费的&lt;strong&gt;专有软件&lt;/strong&gt;，并不开放源代码。&lt;/p&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;
&lt;p&gt;官网的&lt;a class=&#34;link&#34; href=&#34;https://lmstudio.ai/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载页面&lt;/a&gt;提供Windows、Linux和MacOS的下载，除了x86还支持arm64架构的CPU。
安装包是离线的，包含需要的各类运行库，安装过程完全无需联网。
第一次运行的时候会提示选项级别，默认的 &lt;code&gt;Power User&lt;/code&gt; 即可。&lt;/p&gt;
&lt;h2 id=&#34;准备模型&#34;&gt;准备模型&lt;/h2&gt;
&lt;p&gt;LM Studio 支持从&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;huggingface.co&lt;/a&gt;搜索并直接下载运行GGUF格式的 LLM，即量化压缩后的LLM。
但本文的重点是&lt;strong&gt;完全离线运行&lt;/strong&gt;，当然应避免联网功能。&lt;/p&gt;
&lt;p&gt;一个理所当然的方法是下载GGUF模型文件放置在 LM Studio 的模型文件夹 &lt;code&gt;~\.lmstudio\models\&lt;/code&gt; 中，不过这样并不能直接被软件识别。
LM Studio 的解析方式是按照 Hugging Face 模型仓库的 URI 格式来识别模型文件，所以只要按照 &lt;code&gt;~\.lmstudio\models\作者\模型名\参数文件.gguf&lt;/code&gt; 的方式导入 LLM 文件即可。&lt;/p&gt;
&lt;p&gt;作者和模型名可以自己命名，也可以就按照默认方式来，如从 &lt;code&gt;https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/blob/main/mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/code&gt; 下载的文件放在 &lt;code&gt;~\.lmstudio\models\bartowski\mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF\mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将模型文件放置完成后，打开 LM Studio 点击左侧文件夹图表的 &lt;code&gt;Models&lt;/code&gt; 标签页即可看到下载的文件。
列表中还会列出模型的各项参数和功能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://localfreedom.github.io/img/lm-studio.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;chat ui&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;对话&#34;&gt;对话&lt;/h2&gt;
&lt;p&gt;在打开的初始界面，上方选择模型文件加载，可以配置一些参数，通常默认的就足够好了。
等待加载完成就可以在下方与 LLM 对话了。
目前支持上传图片和附件，只要LLM支持相关功能就可以使用。&lt;/p&gt;
&lt;h2 id=&#34;提供服务&#34;&gt;提供服务&lt;/h2&gt;
&lt;p&gt;LM Studio 可以提供兼容 OpenAI v1 的API服务，可以作为局域网中便捷的LLM基础服务，与各种客户端应用程序配合使用。
点击左侧第二个按钮，进入开发者界面即可开启OpenAI兼容API服务。
默认其监听0.0.0.0的1234端口，图形界面提供复制的IP地址可能不合适。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GPT4ALL</title>
        <link>https://localfreedom.github.io/p/gpt4all/</link>
        <pubDate>Tue, 21 Nov 2023 10:32:17 +0000</pubDate>
        
        <guid>https://localfreedom.github.io/p/gpt4all/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;GPT4All是一个本地软件，用于将大语言模型(LargeLanguageModel)通过浮点优化等方法在本地离线提供服务。&lt;/p&gt;
&lt;p&gt;在ChatGPT大火之后，很多厂商模仿推出了开源或专有的大语言对话模型，但这些模型要么以web服务方式提供要么过于需要计算资源而难以本地使用。&lt;/p&gt;
&lt;p&gt;随着facebook的llama的泄漏，一个不错的离线替代品出现，同时&lt;a class=&#34;link&#34; href=&#34;https://github.com/ggerganov/llama.cpp&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;llama.cpp&lt;/a&gt;项目通过浮点计算优化让笔记本也能勉强运行起大模型来。
GPT4All项目将相关技术和资源整合制作了QT桌面软件，让用户能方便的本地离线运行公开的大语言模型，当然它也提供OpenAI的接口以同时使用线上服务。&lt;/p&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;
&lt;p&gt;官网会提供&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-win64.exe&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Windows&lt;/a&gt;、&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-darwin.dmg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MacOS&lt;/a&gt;、&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-linux.run&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linux&lt;/a&gt;的安装包，但每次安装还是会联网检索信息，无法离线安装，只要官网能访问就可以简单的“下一步”安装完成。
也可以从&lt;a class=&#34;link&#34; href=&#34;https://github.com/nomic-ai/gpt4all&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;源代码&lt;/a&gt;编译。&lt;/p&gt;
&lt;h2 id=&#34;下载模型&#34;&gt;下载模型&lt;/h2&gt;
&lt;p&gt;官方会预配置一些模型可供直接使用，由于网络的封锁这一步可以离线进行。&lt;/p&gt;
&lt;p&gt;模型列表来自官网，下载时会跳转到一个github地址 &lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/models/models.json&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://gpt4all.io/models/models.json&lt;/a&gt;
可以手动下载后放到 &lt;code&gt;~/AppData/Roaming/nomic.ai/models.json&lt;/code&gt;
之后点击界面左侧的 &lt;code&gt;Download&lt;/code&gt; 按钮就可以看到可选模型列表了。&lt;/p&gt;
&lt;p&gt;由于huggingface被彻底封锁，只能手动科学下载。
在&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官网首页&lt;/a&gt;下来可以看到模型介绍，选择想下载的模型，点击下方卡片的箭头按钮就可以下载。
下载后放到&lt;code&gt;~/AppData/Local/nomic.ai/GPT4ALL/&lt;/code&gt;里就能被软件自动识别到。&lt;/p&gt;
&lt;h2 id=&#34;对话&#34;&gt;对话&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://localfreedom.github.io/img/gpt4all.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;UI&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;从上方的菜单选择已经下载好被识别的模型，即可像ChatGPT一样对话。
这些模型通常要求16G的内存即可。
对话效果差强人意，而且基本只支持英文输出。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
