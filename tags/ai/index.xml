<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on 局域自由博客</title>
        <link>http://localhost:1313/tags/ai/</link>
        <description>Recent content in AI on 局域自由博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 12 Sep 2025 06:03:04 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>LibreTranslate</title>
        <link>http://localhost:1313/p/libretranslate/</link>
        <pubDate>Fri, 12 Sep 2025 06:03:04 +0000</pubDate>
        
        <guid>http://localhost:1313/p/libretranslate/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://libretranslate.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LibreTranslate&lt;/a&gt; 是一个轻量、开源的机器翻译服务，可以在本地部署或部署到云服务器，实现低延迟、无外部依赖的翻译系统。
LibreTranslate 自身是 &lt;a class=&#34;link&#34; href=&#34;https://www.argosopentech.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Argos Translate&lt;/a&gt; 官方推荐的 WebUI，而 Argos Translate 则基于 &lt;a class=&#34;link&#34; href=&#34;https://opennmt.net/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OpenNMT&lt;/a&gt; 开源架构训练翻译模型。&lt;/p&gt;
&lt;p&gt;相比于如今火爆的大语言模型LLM提供的翻译，LibreTranslate提供的传统的轻巧的翻译服务更加便捷，无需独立显卡，只占用少量内存，尤其便于在个人终端上配合LLM使用，比如生成图片的LLM通常基于英文提示词，且可能直接耗尽显存无力在笔电上再起其他LLM用于翻译，那么使用LibreTranslate即可方便的将中文想法转换为英文提示词。
LibreTranslate 官方提供Container部署方式，可以更加轻易的完成本地部署，本文聚焦使用Docker在个人设备部署可离线使用的轻便翻译服务。&lt;/p&gt;
&lt;h2 id=&#34;docker环境&#34;&gt;Docker环境
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/desktop/release-notes/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载&lt;/a&gt;并安装Docker可以参见前一篇博文。&lt;/p&gt;
&lt;h2 id=&#34;libretranslate镜像&#34;&gt;LibreTranslate镜像
&lt;/h2&gt;&lt;p&gt;从v1.6.3版本开始LibreTranslate镜像的体积异常变大，不知道是打包过程出了什么bug。
本着轻量级可靠的原则，这里推荐v1.6.2和v1.7.2版本，寻求更新的版本建议先在 &lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/r/libretranslate/libretranslate/tags&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DockerHub&lt;/a&gt; 上看看镜像尺寸。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 联网Pull镜像
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker pull docker.io/libretranslate/libretranslate:v1.6.2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 导出镜像可迁往离线环境
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker save -o LibreTranslate.tar libretranslate:v1.6.2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;下载翻译模型&#34;&gt;下载翻译模型
&lt;/h2&gt;&lt;p&gt;LibreTranslate只是WebUI，Docker镜像中集成了 Argos Translate 翻译引擎，工作所需的翻译模型需要单独下载。
Argos官方&lt;a class=&#34;link&#34; href=&#34;https://www.argosopentech.com/argospm/index/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;列出了所有支持的语言翻译模型&lt;/a&gt;。从结构上可以看出此工具是通过英文过渡的方式实现不同语言之间的翻译的，选择自己需要的语言和英语之间的翻译包下载。
下载的 &lt;code&gt;.argosmodel&lt;/code&gt; 格式翻译模型包存放到一个资料夹中，如 &lt;code&gt;C:\Users\UserName\Downloads\MTpackages&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;启动服务&#34;&gt;启动服务
&lt;/h2&gt;&lt;p&gt;在个人设备上装好Docker打开命令行，我们需要将下载的翻译模型包通过卷的方式挂载到容器内，由于默认联网检查并下载所有翻译模型，所以需要通过 &amp;ndash;load-only 限制只加载已下载好的翻译语言模型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 导入镜像(假设放在U盘Z:中)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;docker&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;load&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;LibreTranslate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tar&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 从镜像启动容器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;docker&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;run&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5000&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;Users&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;UserName&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;Downloads&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;MTpackages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;home&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;libretranslate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;local&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;share&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argos&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;translate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;packages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;app&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;LibreTranslate&amp;#34;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;libretranslate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;libretranslate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;only&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;en&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zh&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;打开浏览器，在地址栏输入 &lt;code&gt;localhost:5000&lt;/code&gt; 即可访问WebUI，体验LibreTranslate的机器翻译服务。如果5000端口被其他程式占用了，修改 -p 参数的第一个5000为其他空闲端口号即可。&lt;/p&gt;
&lt;h2 id=&#34;持久运行&#34;&gt;持久运行
&lt;/h2&gt;&lt;p&gt;首次启动后，LibreTranslate会将翻译包解压到同一个资料夹，之后再 -v 挂载可以加上 :ro 选项限制容器写入存储翻译模型包的资料夹。
从命令行启动服务后，即可在 DockerDesktop 的图形界面中停止/启动这一服务，之前的启动命令当然也可以在图形界面配置，但似乎图形界面的配置还不是很完善。&lt;/p&gt;
&lt;h2 id=&#34;效果体验&#34;&gt;效果体验
&lt;/h2&gt;&lt;p&gt;毕竟轻量级，不是大语言模型，但对于配合其他LLM等重型应用，是非常充足的。
翻译结果可能有些机器味，结合个人理解润色一下即可，毕竟用LLM画图的你英文不会太差的。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LM Studio</title>
        <link>http://localhost:1313/p/lm-studio/</link>
        <pubDate>Thu, 27 Mar 2025 10:03:25 +0000</pubDate>
        
        <guid>http://localhost:1313/p/lm-studio/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt;LM Studio 是一个桌面应用程序，它允许你轻松下载、管理和运行各种GGUF格式的开源 LLM，无需连接互联网。
它提供了一个非常友好的配置界面，让你能够探索huggingface，快速体验不同模型的特性，并根据自己的需求进行调整。
在对各类新型模型的支持中，LM Studio 总是非常迅速完美的。
不过 LM Studio 是一个免费的&lt;strong&gt;专有软件&lt;/strong&gt;，并不开放源代码。&lt;/p&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装
&lt;/h2&gt;&lt;p&gt;官网的&lt;a class=&#34;link&#34; href=&#34;https://lmstudio.ai/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载页面&lt;/a&gt;提供Windows、Linux和MacOS的下载，除了x86还支持arm64架构的CPU。
安装包是离线的，包含需要的各类运行库，安装过程完全无需联网。
第一次运行的时候会提示选项级别，默认的 &lt;code&gt;Power User&lt;/code&gt; 即可。&lt;/p&gt;
&lt;h2 id=&#34;准备模型&#34;&gt;准备模型
&lt;/h2&gt;&lt;p&gt;LM Studio 支持从&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;huggingface.co&lt;/a&gt;搜索并直接下载运行GGUF格式的 LLM，即量化压缩后的LLM。
但本文的重点是&lt;strong&gt;完全离线运行&lt;/strong&gt;，当然应避免联网功能。&lt;/p&gt;
&lt;p&gt;一个理所当然的方法是下载GGUF模型文件放置在 LM Studio 的模型文件夹 &lt;code&gt;~\.lmstudio\models\&lt;/code&gt; 中，不过这样并不能直接被软件识别。
LM Studio 的解析方式是按照 Hugging Face 模型仓库的 URI 格式来识别模型文件，所以只要按照 &lt;code&gt;~\.lmstudio\models\作者\模型名\参数文件.gguf&lt;/code&gt; 的方式导入 LLM 文件即可。&lt;/p&gt;
&lt;p&gt;作者和模型名可以自己命名，也可以就按照默认方式来，如从 &lt;code&gt;https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/blob/main/mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/code&gt; 下载的文件放在 &lt;code&gt;~\.lmstudio\models\bartowski\mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF\mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将模型文件放置完成后，打开 LM Studio 点击左侧文件夹图表的 &lt;code&gt;Models&lt;/code&gt; 标签页即可看到下载的文件。
列表中还会列出模型的各项参数和功能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/lm-studio.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;chat ui&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;对话&#34;&gt;对话
&lt;/h2&gt;&lt;p&gt;在打开的初始界面，上方选择模型文件加载，可以配置一些参数，通常默认的就足够好了。
等待加载完成就可以在下方与 LLM 对话了。
目前支持上传图片和附件，只要LLM支持相关功能就可以使用。&lt;/p&gt;
&lt;h2 id=&#34;提供服务&#34;&gt;提供服务
&lt;/h2&gt;&lt;p&gt;LM Studio 可以提供兼容 OpenAI v1 的API服务，可以作为局域网中便捷的LLM基础服务，与各种客户端应用程序配合使用。
点击左侧第二个按钮，进入开发者界面即可开启OpenAI兼容API服务。
默认其监听0.0.0.0的1234端口，图形界面提供复制的IP地址可能不合适。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GPT4ALL</title>
        <link>http://localhost:1313/p/gpt4all/</link>
        <pubDate>Tue, 21 Nov 2023 10:32:17 +0000</pubDate>
        
        <guid>http://localhost:1313/p/gpt4all/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt;GPT4All是一个本地软件，用于将大语言模型(LargeLanguageModel)通过浮点优化等方法在本地离线提供服务。&lt;/p&gt;
&lt;p&gt;在ChatGPT大火之后，很多厂商模仿推出了开源或专有的大语言对话模型，但这些模型要么以web服务方式提供要么过于需要计算资源而难以本地使用。&lt;/p&gt;
&lt;p&gt;随着facebook的llama的泄漏，一个不错的离线替代品出现，同时&lt;a class=&#34;link&#34; href=&#34;https://github.com/ggerganov/llama.cpp&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;llama.cpp&lt;/a&gt;项目通过浮点计算优化让笔记本也能勉强运行起大模型来。
GPT4All项目将相关技术和资源整合制作了QT桌面软件，让用户能方便的本地离线运行公开的大语言模型，当然它也提供OpenAI的接口以同时使用线上服务。&lt;/p&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装
&lt;/h2&gt;&lt;p&gt;官网会提供&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-win64.exe&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Windows&lt;/a&gt;、&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-darwin.dmg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MacOS&lt;/a&gt;、&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-linux.run&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linux&lt;/a&gt;的安装包，但每次安装还是会联网检索信息，无法离线安装，只要官网能访问就可以简单的“下一步”安装完成。
也可以从&lt;a class=&#34;link&#34; href=&#34;https://github.com/nomic-ai/gpt4all&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;源代码&lt;/a&gt;编译。&lt;/p&gt;
&lt;h2 id=&#34;下载模型&#34;&gt;下载模型
&lt;/h2&gt;&lt;p&gt;官方会预配置一些模型可供直接使用，由于网络的封锁这一步可以离线进行。&lt;/p&gt;
&lt;p&gt;模型列表来自官网，下载时会跳转到一个github地址 &lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/models/models.json&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://gpt4all.io/models/models.json&lt;/a&gt;
可以手动下载后放到 &lt;code&gt;~/AppData/Roaming/nomic.ai/models.json&lt;/code&gt;
之后点击界面左侧的 &lt;code&gt;Download&lt;/code&gt; 按钮就可以看到可选模型列表了。&lt;/p&gt;
&lt;p&gt;由于huggingface被彻底封锁，只能手动科学下载。
在&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官网首页&lt;/a&gt;下来可以看到模型介绍，选择想下载的模型，点击下方卡片的箭头按钮就可以下载。
下载后放到&lt;code&gt;~/AppData/Local/nomic.ai/GPT4ALL/&lt;/code&gt;里就能被软件自动识别到。&lt;/p&gt;
&lt;h2 id=&#34;对话&#34;&gt;对话
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/gpt4all.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;UI&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;从上方的菜单选择已经下载好被识别的模型，即可像ChatGPT一样对话。
这些模型通常要求16G的内存即可。
对话效果差强人意，而且基本只支持英文输出。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
