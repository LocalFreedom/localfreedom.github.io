<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on 局域自由博客</title>
        <link>https://localfreedom.github.io/tags/ai/</link>
        <description>Recent content in AI on 局域自由博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 04 Oct 2025 06:11:21 +0000</lastBuildDate><atom:link href="https://localfreedom.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Ollama模型部署</title>
        <link>https://localfreedom.github.io/p/ollama%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/</link>
        <pubDate>Sat, 04 Oct 2025 06:11:21 +0000</pubDate>
        
        <guid>https://localfreedom.github.io/p/ollama%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/</guid>
        <description>&lt;img src="https://localfreedom.github.io/img/ollama-cloud.png" alt="Featured image of post Ollama模型部署" /&gt;&lt;p&gt;Ollama是一个流行的LLM本地部署推理软件，从其&lt;a class=&#34;link&#34; href=&#34;https://github.com/ollama/ollama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github仓库&lt;/a&gt;可以下载安装程式或直接运行其&lt;a class=&#34;link&#34; href=&#34;https://docker.io/ollama/ollama&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Docker映像&lt;/a&gt;。
本文介绍将&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/models&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;HuggingFace上的模型&lt;/a&gt;导入离线环境中的Ollama部署推理的3种方法。&lt;/p&gt;
&lt;h2 id=&#34;导入gguf格式&#34;&gt;导入GGUF格式
&lt;/h2&gt;&lt;p&gt;创建一个modelfile文档，尽量和模型文档在同一个文档夹。
格式分为三部分:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;模型权重文档路径，以FROM开头，多模态可以有多行。&lt;/li&gt;
&lt;li&gt;对话模板，以TEMPLATE开头用引号包裹，配置提示词等。&lt;/li&gt;
&lt;li&gt;参数，以PARAMETER开头每行1个，配置温度、停止词等。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;模板和参数建议使用&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/library&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama官方&lt;/a&gt;同名模型的配置，通常不同尺寸和量化精度的配置是一样的。&lt;/p&gt;
&lt;p&gt;例如Google的Gemma3模型12b尺寸的配置文档:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;FROM gemma-3-12b-it-q4_0.gguf
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;FROM mmproj-google_gemma-3-12b-it-qat-f16.gguf
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;TEMPLATE &amp;#34;{{- range $i, $_ := .Messages }}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{- $last := eq (len (slice $.Messages $i)) 1 }}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{- if or (eq .Role &amp;#34;user&amp;#34;) (eq .Role &amp;#34;system&amp;#34;) }}&amp;lt;start_of_turn&amp;gt;user
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{ .Content }}&amp;lt;end_of_turn&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{ if $last }}&amp;lt;start_of_turn&amp;gt;model
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{ end }}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{- else if eq .Role &amp;#34;assistant&amp;#34; }}&amp;lt;start_of_turn&amp;gt;model
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{ .Content }}{{ if not $last }}&amp;lt;end_of_turn&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{ end }}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{- end }}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{- end }}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;PARAMETER stop &amp;lt;end_of_turn&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;PARAMETER temperature 1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;PARAMETER top_k 64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;PARAMETER top_p 0.95
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;执行命令 &lt;code&gt;ollama create 模型名 -f 配置文档.modelfile&lt;/code&gt;
ollama就会将GGUF格式的模型文档导入成内部数据。&lt;/p&gt;
&lt;h2 id=&#34;迁移已有模型&#34;&gt;迁移已有模型
&lt;/h2&gt;&lt;p&gt;Ollama存储模型分为配置索引和数据块两部分。
将已经下载部署好的模型迁移到新安装环境，只需要复制配置文档和数据块即可。
模型配置文档根据路径可以看出保存在 &lt;code&gt;.ollama/models/manifests/registry.ollama.ai/library/模型名:参数&lt;/code&gt; 中，其中模型名如gemma3，参数如12b-it-qat，也就是run时冒号前后的部分。
打开配置文档是json格式，找到其中的&lt;code&gt;sha256:xxx...&lt;/code&gt;就是对应的数据块，在 &lt;code&gt;.ollama/models/blobs/&lt;/code&gt; 中找到相应的文档便是该模型的数据块。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;schemaVersion&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;mediaType&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;application/vnd.docker.distribution.manifest.v2+json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;config&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;mediaType&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;application/vnd.docker.container.image.v1+json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;digest&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sha256:505f8883abf333a307ed4120ef5f3116f442e6c537e5c4332d9044492e2d637c&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;414&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;layers&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:[{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;mediaType&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;application/vnd.ollama.image.model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;digest&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sha256:1fb99eda86dc48a736567406253769fdc75f01e65cde7c65fa5563e4bdf156e0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8928674496&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;from&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/Users/pdevine/.ollama/models/blobs/sha256-1fb99eda86dc48a736567406253769fdc75f01e65cde7c65fa5563e4bdf156e0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;mediaType&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;application/vnd.ollama.image.template&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;digest&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sha256:5be6630b666f007a127a456fb3958da5fbd7cf547204188bc4690fedc837af0b&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;359&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;mediaType&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;application/vnd.ollama.image.params&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;digest&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sha256:3116c52250752e00dd06b16382e952bd33c34fd79fc4fe3a5d2c77cf7de1b14b&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;size&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;77&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;将配置文档和数据块复制到新安装环境即可正常使用之前下载好的模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;数据块有大有小，大的就是下载的LLM参数文档，小的可以打开看，通常是模板和参数，所以复制后模板等配置也原封不动。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;离线下载模型文档&#34;&gt;离线下载模型文档
&lt;/h2&gt;&lt;p&gt;有时候需要下载Ollama的官方模型，但并不便于安装一个Ollama，就需要直接通过URL下载模型配置和权重文档。
虽然目前官方没有在文档中说明下载链接格式，但是对于run时的 &lt;code&gt;模型名:参数&lt;/code&gt; 可以根据规则推测其直链。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置文档如 &lt;code&gt;https://registry.ollama.ai/v2/library/模型名/manifests/参数&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;权重数据块文档如 &lt;code&gt;https://registry.ollama.ai/v2/library/模型名/blobs/sha256:配置文档中的数据块散列值&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/Gholamrezadar/ollama-direct-downloader&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Gholamreza Dar&lt;/a&gt;制作了&lt;a class=&#34;link&#34; href=&#34;https://ollama-direct-downloader.vercel.app/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;提取Ollama直链的网页&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下载好配置文档和权重数据块后，按照规律放到个人文档夹中 &lt;code&gt;.ollama/models/&lt;/code&gt; 相应位置即可被Ollama运行时调用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置文档如 &lt;code&gt;.ollama/models/manifests/registry.ollama.ai/library/模型名:参数&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;权重数据块文档如 &lt;code&gt;.ollama/models/blobs/sha256:配置文档中的数据块散列值&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>LibreTranslate</title>
        <link>https://localfreedom.github.io/p/libretranslate/</link>
        <pubDate>Fri, 12 Sep 2025 06:03:04 +0000</pubDate>
        
        <guid>https://localfreedom.github.io/p/libretranslate/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://libretranslate.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LibreTranslate&lt;/a&gt; 是一个轻量、开源的机器翻译服务，可以在本地部署或部署到云服务器，实现低延迟、无外部依赖的翻译系统。
LibreTranslate 自身是 &lt;a class=&#34;link&#34; href=&#34;https://www.argosopentech.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Argos Translate&lt;/a&gt; 官方推荐的 WebUI，而 Argos Translate 则基于 &lt;a class=&#34;link&#34; href=&#34;https://opennmt.net/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;OpenNMT&lt;/a&gt; 开源架构训练翻译模型。&lt;/p&gt;
&lt;p&gt;相比于如今火爆的大语言模型LLM提供的翻译，LibreTranslate提供的传统的轻巧的翻译服务更加便捷，无需独立显卡，只占用少量内存，尤其便于在个人终端上配合LLM使用，比如生成图片的LLM通常基于英文提示词，且可能直接耗尽显存无力在笔电上再起其他LLM用于翻译，那么使用LibreTranslate即可方便的将中文想法转换为英文提示词。
LibreTranslate 官方提供Container部署方式，可以更加轻易的完成本地部署，本文聚焦使用Docker在个人设备部署可离线使用的轻便翻译服务。&lt;/p&gt;
&lt;h2 id=&#34;docker环境&#34;&gt;Docker环境
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://docs.docker.com/desktop/release-notes/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载&lt;/a&gt;并安装Docker可以参见前一篇博文。&lt;/p&gt;
&lt;h2 id=&#34;libretranslate镜像&#34;&gt;LibreTranslate镜像
&lt;/h2&gt;&lt;p&gt;从v1.6.3版本开始LibreTranslate镜像的体积异常变大，不知道是打包过程出了什么bug。
本着轻量级可靠的原则，这里推荐v1.6.2和v1.7.2版本，寻求更新的版本建议先在 &lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/r/libretranslate/libretranslate/tags&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DockerHub&lt;/a&gt; 上看看镜像尺寸。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 联网Pull镜像
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker pull docker.io/libretranslate/libretranslate:v1.6.2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# 导出镜像可迁往离线环境
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker save -o LibreTranslate.tar libretranslate:v1.6.2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;下载翻译模型&#34;&gt;下载翻译模型
&lt;/h2&gt;&lt;p&gt;LibreTranslate只是WebUI，Docker镜像中集成了 Argos Translate 翻译引擎，工作所需的翻译模型需要单独下载。
Argos官方&lt;a class=&#34;link&#34; href=&#34;https://www.argosopentech.com/argospm/index/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;列出了所有支持的语言翻译模型&lt;/a&gt;。从结构上可以看出此工具是通过英文过渡的方式实现不同语言之间的翻译的，选择自己需要的语言和英语之间的翻译包下载。
下载的 &lt;code&gt;.argosmodel&lt;/code&gt; 格式翻译模型包存放到一个资料夹中，如 &lt;code&gt;C:\Users\UserName\Downloads\MTpackages&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;启动服务&#34;&gt;启动服务
&lt;/h2&gt;&lt;p&gt;在个人设备上装好Docker打开命令行，我们需要将下载的翻译模型包通过卷的方式挂载到容器内，由于默认联网检查并下载所有翻译模型，所以需要通过 &amp;ndash;load-only 限制只加载已下载好的翻译语言模型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 导入镜像(假设放在U盘Z:中)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;docker&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;load&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;LibreTranslate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tar&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 从镜像启动容器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;docker&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;run&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5000&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;Users&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;UserName&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;Downloads&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;MTpackages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;home&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;libretranslate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;local&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;share&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argos&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;translate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;packages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;app&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;LibreTranslate&amp;#34;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;libretranslate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;libretranslate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;only&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;en&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zh&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;打开浏览器，在地址栏输入 &lt;code&gt;localhost:5000&lt;/code&gt; 即可访问WebUI，体验LibreTranslate的机器翻译服务。如果5000端口被其他程式占用了，修改 -p 参数的第一个5000为其他空闲端口号即可。&lt;/p&gt;
&lt;h2 id=&#34;持久运行&#34;&gt;持久运行
&lt;/h2&gt;&lt;p&gt;首次启动后，LibreTranslate会将翻译包解压到同一个资料夹，之后再 -v 挂载可以加上 :ro 选项限制容器写入存储翻译模型包的资料夹。
从命令行启动服务后，即可在 DockerDesktop 的图形界面中停止/启动这一服务，之前的启动命令当然也可以在图形界面配置，但似乎图形界面的配置还不是很完善。&lt;/p&gt;
&lt;h2 id=&#34;效果体验&#34;&gt;效果体验
&lt;/h2&gt;&lt;p&gt;毕竟轻量级，不是大语言模型，但对于配合其他LLM等重型应用，是非常充足的。
翻译结果可能有些机器味，结合个人理解润色一下即可，毕竟用LLM画图的你英文不会太差的。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LM Studio</title>
        <link>https://localfreedom.github.io/p/lm-studio/</link>
        <pubDate>Thu, 27 Mar 2025 10:03:25 +0000</pubDate>
        
        <guid>https://localfreedom.github.io/p/lm-studio/</guid>
        <description>&lt;img src="https://localfreedom.github.io/img/lmstudio-app-logo.webp" alt="Featured image of post LM Studio" /&gt;&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt;LM Studio 是一个桌面应用程序，它允许你轻松下载、管理和运行各种GGUF格式的开源 LLM，无需连接互联网。
它提供了一个非常友好的配置界面，让你能够探索huggingface，快速体验不同模型的特性，并根据自己的需求进行调整。
在对各类新型模型的支持中，LM Studio 总是非常迅速完美的。
不过 LM Studio 是一个免费的&lt;strong&gt;专有软件&lt;/strong&gt;，并不开放源代码。&lt;/p&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装
&lt;/h2&gt;&lt;p&gt;官网的&lt;a class=&#34;link&#34; href=&#34;https://lmstudio.ai/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;下载页面&lt;/a&gt;提供Windows、Linux和MacOS的下载，除了x86还支持arm64架构的CPU。
安装包是离线的，包含需要的各类运行库，安装过程完全无需联网。
第一次运行的时候会提示选项级别，默认的 &lt;code&gt;Power User&lt;/code&gt; 即可。&lt;/p&gt;
&lt;h2 id=&#34;准备模型&#34;&gt;准备模型
&lt;/h2&gt;&lt;p&gt;LM Studio 支持从&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;huggingface.co&lt;/a&gt;搜索并直接下载运行GGUF格式的 LLM，即量化压缩后的LLM。
但本文的重点是&lt;strong&gt;完全离线运行&lt;/strong&gt;，当然应避免联网功能。&lt;/p&gt;
&lt;p&gt;一个理所当然的方法是下载GGUF模型文件放置在 LM Studio 的模型文件夹 &lt;code&gt;~\.lmstudio\models\&lt;/code&gt; 中，不过这样并不能直接被软件识别。
LM Studio 的解析方式是按照 Hugging Face 模型仓库的 URI 格式来识别模型文件，所以只要按照 &lt;code&gt;~\.lmstudio\models\作者\模型名\参数文件.gguf&lt;/code&gt; 的方式导入 LLM 文件即可。&lt;/p&gt;
&lt;p&gt;作者和模型名可以自己命名，也可以就按照默认方式来，如从 &lt;code&gt;https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF/blob/main/mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/code&gt; 下载的文件放在 &lt;code&gt;~\.lmstudio\models\bartowski\mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF\mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将模型文件放置完成后，打开 LM Studio 点击左侧文件夹图表的 &lt;code&gt;Models&lt;/code&gt; 标签页即可看到下载的文件。
列表中还会列出模型的各项参数和功能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://localfreedom.github.io/img/lm-studio.webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;chat ui&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;对话&#34;&gt;对话
&lt;/h2&gt;&lt;p&gt;在打开的初始界面，上方选择模型文件加载，可以配置一些参数，通常默认的就足够好了。
等待加载完成就可以在下方与 LLM 对话了。
目前支持上传图片和附件，只要LLM支持相关功能就可以使用。&lt;/p&gt;
&lt;h2 id=&#34;提供服务&#34;&gt;提供服务
&lt;/h2&gt;&lt;p&gt;LM Studio 可以提供兼容 OpenAI v1 的API服务，可以作为局域网中便捷的LLM基础服务，与各种客户端应用程序配合使用。
点击左侧第二个按钮，进入开发者界面即可开启OpenAI兼容API服务。
默认其监听0.0.0.0的1234端口，图形界面提供复制的IP地址可能不合适。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GPT4ALL</title>
        <link>https://localfreedom.github.io/p/gpt4all/</link>
        <pubDate>Tue, 21 Nov 2023 10:32:17 +0000</pubDate>
        
        <guid>https://localfreedom.github.io/p/gpt4all/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt;GPT4All是一个本地软件，用于将大语言模型(LargeLanguageModel)通过浮点优化等方法在本地离线提供服务。&lt;/p&gt;
&lt;p&gt;在ChatGPT大火之后，很多厂商模仿推出了开源或专有的大语言对话模型，但这些模型要么以web服务方式提供要么过于需要计算资源而难以本地使用。&lt;/p&gt;
&lt;p&gt;随着facebook的llama的泄漏，一个不错的离线替代品出现，同时&lt;a class=&#34;link&#34; href=&#34;https://github.com/ggerganov/llama.cpp&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;llama.cpp&lt;/a&gt;项目通过浮点计算优化让笔记本也能勉强运行起大模型来。
GPT4All项目将相关技术和资源整合制作了QT桌面软件，让用户能方便的本地离线运行公开的大语言模型，当然它也提供OpenAI的接口以同时使用线上服务。&lt;/p&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装
&lt;/h2&gt;&lt;p&gt;官网会提供&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-win64.exe&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Windows&lt;/a&gt;、&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-darwin.dmg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MacOS&lt;/a&gt;、&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/installers/gpt4all-installer-linux.run&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linux&lt;/a&gt;的安装包，但每次安装还是会联网检索信息，无法离线安装，只要官网能访问就可以简单的“下一步”安装完成。
也可以从&lt;a class=&#34;link&#34; href=&#34;https://github.com/nomic-ai/gpt4all&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;源代码&lt;/a&gt;编译。&lt;/p&gt;
&lt;h2 id=&#34;下载模型&#34;&gt;下载模型
&lt;/h2&gt;&lt;p&gt;官方会预配置一些模型可供直接使用，由于网络的封锁这一步可以离线进行。&lt;/p&gt;
&lt;p&gt;模型列表来自官网，下载时会跳转到一个github地址 &lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/models/models.json&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://gpt4all.io/models/models.json&lt;/a&gt;
可以手动下载后放到 &lt;code&gt;~/AppData/Roaming/nomic.ai/models.json&lt;/code&gt;
之后点击界面左侧的 &lt;code&gt;Download&lt;/code&gt; 按钮就可以看到可选模型列表了。&lt;/p&gt;
&lt;p&gt;由于huggingface被彻底封锁，只能手动科学下载。
在&lt;a class=&#34;link&#34; href=&#34;https://gpt4all.io/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官网首页&lt;/a&gt;下来可以看到模型介绍，选择想下载的模型，点击下方卡片的箭头按钮就可以下载。
下载后放到&lt;code&gt;~/AppData/Local/nomic.ai/GPT4ALL/&lt;/code&gt;里就能被软件自动识别到。&lt;/p&gt;
&lt;h2 id=&#34;对话&#34;&gt;对话
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://localfreedom.github.io/img/gpt4all.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;UI&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;从上方的菜单选择已经下载好被识别的模型，即可像ChatGPT一样对话。
这些模型通常要求16G的内存即可。
对话效果差强人意，而且基本只支持英文输出。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
